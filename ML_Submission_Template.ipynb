{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KiranKote12/Zomato_resturent_Clustering_and_Sentiment_Analysis.ipynb/blob/main/ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font size='8px'><font color='#FF3206'> **Zomato Resturent Clustering and Sentiment Analysis**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual/Team\n",
        "##### **Team Member 1 -**  Kiran Kote\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zomato is an Indian restaurant aggregator and food delivery start-up founded by Deepinder Goyal and Pankaj Chaddah in 2008. Zomato provides information, menus and user-reviews of restaurants, and also has food delivery options from partner restaurants in select cities.\n",
        "\n",
        "India is quite famous for its diverse multi cuisine available in a large number of restaurants and hotel resorts, which is reminiscent of unity in diversity. Restaurant business in India is always evolving. More Indians are warming up to the idea of eating restaurant food whether by dining outside or getting food delivered. The growing number of restaurants in every state of India has been a motivation to inspect the data to get some insights, interesting facts and figures about the Indian food industry in each city. So, this project focuses on analysing the Zomato restaurant data for each city in India.\n",
        "\n",
        "The Project focuses on Customers and Company, you have to analyze the sentiments of the reviews given by the customer in the data and made some useful conclusion in the form of Visualizations. Also, cluster the zomato restaurants into different segments. The data is vizualized as it becomes easy to analyse data at instant. The Analysis also solve some of the business cases that can directly help the customers finding the Best restaurant in their locality and for the company to grow up and work on the fields they are currently lagging in.\n",
        "\n",
        "This could help in clustering the restaurants into segments. Also the data has valuable information around cuisine and costing which can be used in cost vs. benefit analysis\n",
        "\n",
        "Data could be used for sentiment analysis. Also the metadata of reviewers can be used for identifying the critics in the industry."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/KiranKote12/Zomato_resturent_Clustering_and_Sentiment_Analysis.ipynb"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import Visuliztion Library\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "\n",
        "# Annomly Detection Tools\n",
        "from sklearn.ensemble import IsolationForest"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Resturent Dataset\n",
        "resturent_df=pd.read_csv('/content/drive/MyDrive/Capstone_Projects/zomato capstone project/data/Zomato Restaurant names and Metadata.csv')\n",
        "# Load Review Dataset\n",
        "review_df = pd.read_csv('/content/drive/MyDrive/Capstone_Projects/zomato capstone project/data/Zomato Restaurant reviews.csv')"
      ],
      "metadata": {
        "id": "WnFJKKWfqEgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "resturent_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.head()"
      ],
      "metadata": {
        "id": "WnrgNPaRs33b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f' The number of Row and Column int the  Resturent Dataset {resturent_df.shape}')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Row & Columns count in Review Dataset\n",
        "print(f'The number of Row and Column in the Review Data  {review_df.shape}')"
      ],
      "metadata": {
        "id": "5bn2rBoPtjcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHwYmJAmNHm"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsikbmjC0H5i"
      },
      "source": [
        "All the Variables persent in Zomato Restaurant names and Metadata\n",
        "\n",
        "1. Name : Name of Restaurants\n",
        "\n",
        "2. Links : URL Links of Restaurants\n",
        "\n",
        "3. Cost : Per person estimated Cost for Dinning\n",
        "\n",
        "4. Collection : Tagging of Restaurants w.r.t. Zomato categories\n",
        "\n",
        "5. Cuisines : Cuisines(Type of food) served by Restaurants\n",
        "\n",
        "6. Timings : Restaurant Timings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K2FGww21m35"
      },
      "source": [
        "All the Variables persent in Zomato Review Dataset\n",
        "\n",
        "1. Restaurant : Name of the Restaurant\n",
        "\n",
        "2. Reviewer : Name of the Reviewer\n",
        "\n",
        "3. Review : Review Text\n",
        "\n",
        "4. Rating : Rating Provided by Reviewer\n",
        "\n",
        "5. MetaData : Reviewer Metadata - No. of Reviews and followers\n",
        "\n",
        "6. Time: Date and Time of Review\n",
        "\n",
        "7. Pictures : No. of pictures posted with review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "outputs": [],
      "source": [
        "# Dataset Information of Resturent\n",
        "resturent_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NEgqxLSFgXN"
      },
      "outputs": [],
      "source": [
        "# Dataset Information of Review\n",
        "review_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35m5QtbWiB9F"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "outputs": [],
      "source": [
        "# Dataset Duplicate Value Count resturent columns\n",
        "print(f'Number of duplicte item persent in Resturent dataset =  {resturent_df.duplicated().sum()} \\n')\n",
        "\n",
        "print('--'*50)\n",
        "\n",
        "\n",
        "# Dataset Duplicate Values Count review column\n",
        "print(f'Number of duplicte item persent in Review dataset = {review_df.duplicated().sum()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPl-ycgm1ru"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "outputs": [],
      "source": [
        "# Missing Values/Null Values\n",
        "print('---Null values persent int the Resturent dataset---')\n",
        "print(resturent_df.isnull().sum(), '\\n')\n",
        "\n",
        "print('--'*50)\n",
        "\n",
        "# Missing Values/Null Values Count\n",
        "print('---Null values persent int the Resturent dataset---')\n",
        "print(review_df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUB9qStmQy4V"
      },
      "outputs": [],
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(resturent_df.isnull(), cbar=True , yticklabels= False)\n",
        "plt.xlabel('Name of columns ', size =15, weight='bold')\n",
        "plt.title('All the Missing values persent in Resturent columns are \\n', fontweight= 'bold', size=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW-ckkcU3TW2"
      },
      "outputs": [],
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(review_df.isnull(), cbar=True , yticklabels= False)\n",
        "plt.xlabel('Name of columns ', size =15, weight='bold')\n",
        "plt.title('All the Missing values persent in Review columns are \\n', fontweight= 'bold', size=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restaurant DataSet**\n",
        "* There are 105 total observation with 6 different Variable/features.\n",
        "\n",
        "* Collection Variable have high volume of null values persent and timing also has null values.\n",
        "\n",
        "* There is no duplicate value persent in the resturent dataset.\n",
        "\n",
        "* Cost feature represent the amount but it shows object type because it is saperated by ',' comma\n",
        "\n",
        "* Timing represent operational hour but as it is represented in the form of text has object data type.\n",
        "\n",
        "**Review DataSet**\n",
        "* There are total 10000 observation and 7 Variable/features.\n",
        "\n",
        "* Only picture and restaurant feature does not have null value from .\n",
        "\n",
        "* There are total of 36 duplicate values persent in the review dataset\n",
        "\n",
        "* Rating represent ordinal data, has object data type should be integer.\n",
        "\n",
        "* Timing represent the time when review was posted but show object data time, it should be converted into date time.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dauF4eBmngu3"
      },
      "source": [
        "## 3. ***Data Wrangling***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjF_gweDVsbn"
      },
      "source": [
        "Data wrangling can be defined as the process of cleaning, organizing, and transforming raw data into the desired format for analysts to use for prompt decision-making. Also known as data cleaning or data munging, data wrangling enables businesses to tackle more complex data in less time, produce more accurate results, and make better decisions. The exact methods vary from project to project depending upon your data and the goal you are trying to achieve. More and more organizations are increasingly relying on data wrangling tools to make data ready for downstream analytics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJF3rekwFvQ"
      },
      "source": [
        "### Data Wrangling Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBk1IqRinQ2c"
      },
      "source": [
        "#### Wrangling Resturent  Dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4ZTQrjbzkII"
      },
      "source": [
        "Wrangling on Cost feature/variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E61TkqubncQ7"
      },
      "outputs": [],
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "resturent_df['Cost'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apTigE27F5g1"
      },
      "outputs": [],
      "source": [
        "# Remove the ',' from the cost varible\n",
        "resturent_df['Cost']= resturent_df['Cost'].str.replace(',','').astype('int64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBbkm9FQoEeD"
      },
      "outputs": [],
      "source": [
        "# Find out the  top 10 costly resturent\n",
        "Top_10_Expensive_Restuent=resturent_df.sort_values('Cost', ascending=False)[['Name', 'Cost']][:10]\n",
        "Top_10_Expensive_Restuent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O91rEilmoP95"
      },
      "outputs": [],
      "source": [
        "# Find out 10 economicaly  resturent\n",
        "Top_10_cheapest_resturent=resturent_df.sort_values('Cost', ascending=True)[['Name', 'Cost']][:10]\n",
        "Top_10_cheapest_resturent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8PEqnMnzsa_"
      },
      "source": [
        "Wrangling on Collection feature/variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtqClFZQvaBc"
      },
      "outputs": [],
      "source": [
        "# spliting the Collection and storing in list\n",
        "Collections_list = resturent_df.Collections.dropna().str.split(', ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Daji8gZTtS89"
      },
      "outputs": [],
      "source": [
        "# storing all the cusines in a dict\n",
        "Collections_dict = {}\n",
        "for collection in Collections_list:\n",
        "    for col_name in collection:\n",
        "        if (col_name in Collections_dict):\n",
        "            Collections_dict[col_name]+=1\n",
        "        else:\n",
        "            Collections_dict[col_name]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPTYlIsywYm5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# converting the dict to a data frame\n",
        "Collections_df=pd.DataFrame.from_dict([Collections_dict]).transpose().reset_index().rename(columns={'index':'Tags',0:'Number_of_Restaurants'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkaOaI3bxWKS"
      },
      "outputs": [],
      "source": [
        "#top 10 collection\n",
        "Collections_df.sort_values('Number_of_Restaurants', ascending =False)[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_9rwxZpxk_P"
      },
      "source": [
        "Wrangling on Cuisine variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOor2g-kqA09"
      },
      "outputs": [],
      "source": [
        "# store type of food in a a list with spliting all the food\n",
        "cuisine_list =resturent_df.Cuisines.str.split(', ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBlE9qRlrF3g"
      },
      "outputs": [],
      "source": [
        "# storing all the cusines in a dict\n",
        "cuisine_dict = {}\n",
        "for cuisine_names in cuisine_list:\n",
        "    for cuisine in cuisine_names:\n",
        "        if (cuisine in cuisine_dict):\n",
        "            cuisine_dict[cuisine]+=1\n",
        "        else:\n",
        "            cuisine_dict[cuisine]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_n-xFJSirYP3"
      },
      "outputs": [],
      "source": [
        "# converting the dict to a data frame\n",
        "cuisine_df=pd.DataFrame.from_dict([cuisine_dict]).transpose().reset_index().rename(columns={'index':'Type_of_Food',0:'Number_of_Restaurants'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyLOp84GrdwZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "#top 10 cuisine\n",
        "cusine_of_the_data=cuisine_df.sort_values('Number_of_Restaurants', ascending =False)\n",
        "cusine_of_the_data[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CWfgkdWm-jb"
      },
      "source": [
        "### Review :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEkx9acw4BLm"
      },
      "source": [
        "Wrengling on Rating variable/Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A409ZOnI4PeB"
      },
      "outputs": [],
      "source": [
        "review_df['Rating'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jr42A_DkxsS"
      },
      "outputs": [],
      "source": [
        "# convert Like into nan and type  of data\n",
        "review_df.loc[review_df['Rating']=='Like']=np.nan\n",
        "review_df['Rating']= review_df['Rating'].astype('float')\n",
        "\n",
        "# Fill Null values by mean\n",
        "review_df['Rating'].fillna(3.5, inplace=True)\n",
        "review_df['Rating'].unique()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RanpTmD4HNhp"
      },
      "source": [
        "Wrengling on Metadata variable/Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N23q1_O5HWGg"
      },
      "outputs": [],
      "source": [
        "# split metadata column into 2 columns i.e. Reviews and followers\n",
        "\n",
        "# Use expand=True to create a DataFrame from the split\n",
        "split_df = review_df['Metadata'].str.split(',', expand=True)\n",
        "\n",
        "# Assign the first and second columns to 'Reviews' and 'Followers'\n",
        "review_df['Reviews'] = split_df[0]\n",
        "review_df['Followers'] = split_df[1]\n",
        "\n",
        "# Extract numeric values from 'Reviews' and 'Followers'\n",
        "review_df['Reviews'] = pd.to_numeric(review_df['Reviews'].str.split(' ').str[0])\n",
        "review_df['Followers'] = pd.to_numeric(review_df['Followers'].str.split(' ').str[1])\n",
        "\n",
        "# Drop the Metadata columns\n",
        "review_df = review_df.drop(['Metadata'], axis=1)\n",
        "review_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm8M4EPBJvEF"
      },
      "outputs": [],
      "source": [
        "# Split the Time columns into Year Month and Hours\n",
        "review_df['Year'] = pd.DatetimeIndex(review_df['Time']).year\n",
        "review_df['Month'] = pd.DatetimeIndex(review_df['Time']).month\n",
        "review_df['Hour'] = pd.DatetimeIndex(review_df['Time']).hour\n",
        "review_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciLS13mnLkpC"
      },
      "outputs": [],
      "source": [
        "# code to find top reviewer\n",
        "reviewer_list = review_df.groupby('Reviewer').apply(lambda x: x['Reviewer'].count()).reset_index(name='Review_Count')\n",
        "reviewer_list = reviewer_list.sort_values(by = 'Review_Count',ascending=False)\n",
        "top_10_reviewers = reviewer_list[:10]\n",
        "top_10_reviewers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUgcgjHaORFx"
      },
      "outputs": [],
      "source": [
        "# Average of the ratings of reviewers with review count\n",
        "review_ratings=review_df.groupby('Reviewer').apply(lambda x:np.average(x['Rating'])).reset_index(name='Average_Ratings')\n",
        "review_ratings=pd.merge(top_10_reviewers,review_ratings,how='inner',left_on='Reviewer',right_on='Reviewer')\n",
        "top_10_reviewers_ratings=review_ratings[:10]\n",
        "top_10_reviewers_ratings=top_10_reviewers_ratings.sort_values(by = 'Average_Ratings',ascending=False)\n",
        "top_10_reviewers_ratings.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbSfqDJBTsBz"
      },
      "outputs": [],
      "source": [
        "review_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b82ycfK6TdbW"
      },
      "outputs": [],
      "source": [
        "#Finding the most followed critic\n",
        "most_followed_reviewer = review_df.groupby('Reviews').agg({'Reviewer':'max','Followers':'max', 'Rating':'mean'}).reset_index().rename(columns = {\n",
        "          'Rating':'Average_Rating_Given'}).sort_values('Followers', ascending = False)\n",
        "most_followed_reviewer[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pM78pIWM7YZv"
      },
      "outputs": [],
      "source": [
        "#Average engagement of restaurants\n",
        "avg_hotel_rating = review_df.groupby('Restaurant').agg({'Rating':'mean','Reviewer': 'count'}).reset_index().rename(columns = {'Reviewer': 'Total_Review'})\n",
        "avg_hotel_rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCt59BMZ4sSn"
      },
      "outputs": [],
      "source": [
        "#merging both data frame\n",
        "resturent_df = resturent_df.rename(columns = {'Name':'Restaurant'})\n",
        "merged_df = resturent_df.merge(review_df, on = 'Restaurant')\n",
        "merged_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L2c9w7LCEBb"
      },
      "outputs": [],
      "source": [
        "merged_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSa1f5Uengrz"
      },
      "source": [
        "### What all manipulations have you done and insights you found?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbyXE7I1olp8"
      },
      "source": [
        "Firstly, We have started with changing the datatype of the variable persent in the dataset like cost and rating. this will help to make data consistent.\n",
        "\n",
        "\n",
        "\n",
        "*   We have found 10 Expensive Resturent from the top and top 10 cheapest Resturent.  \n",
        "\n",
        "\n",
        "*   Divide the Metadata variable into two varilable ie Review and Followers\n",
        "\n",
        "\n",
        "*   Top 10 Collection and Top 10 Cuisine  \n",
        "\n",
        "\n",
        "*   Splited the Time Variable into new variable like Year Month and Hours\n",
        "\n",
        "\n",
        "*   We have found top 10 reviewer and the average rating of the reviewer.\n",
        "\n",
        "\n",
        "*   Top 10 most which have highest number of follower\n",
        "\n",
        "\n",
        "*   Merge the Resturent and Review variable with Name and resturent\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF8Ens_Soomf"
      },
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wOQAZs5pc--"
      },
      "source": [
        "#### Chart - 1  Distrbution plot of Cost, Rating, Year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooMYyDuZmqFv"
      },
      "outputs": [],
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize = (18,8));\n",
        "for i,col in enumerate(['Cost','Rating','Year']) :\n",
        "    # plt.figure(figsize = (8,5));\n",
        "    plt.subplot(2,2,i+1);\n",
        "    sns.distplot(merged_df[col], color = '#055E85');\n",
        "    feature = merged_df[col]\n",
        "    plt.axvline(feature.mean(), color='#ff033e', linestyle='dashed', linewidth=3,label= 'mean');  #red\n",
        "    plt.axvline(feature.median(), color='#A020F0', linestyle='dashed', linewidth=3,label='median'); #cyan\n",
        "    plt.legend(bbox_to_anchor = (1.0, 1))\n",
        "    plt.title(f'{col.title()}');\n",
        "    plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S62S3ZCfCY0n"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZVZdPc_CfQc"
      },
      "source": [
        "We have pick up Distplot which is helpful for understanding the distribution of the Distplot is helpful in understanding the distribution of the variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQCqLDYGCg86"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7veW1_n5Cwvp"
      },
      "source": [
        "From the distplot we have found that all three variable persent have some skewness persent into it and Maximum resturent show the price range of 500 and 2018 and 2019 have high volume of review found."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqZD5igDCobb"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InudguGQCp-V"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snMQP739CwUc"
      },
      "source": [
        "From the Given distplot we have found important information ie Cost which makes high impect on business along with rating which show how much the person is engaged with the product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrMuq3yWnqSF"
      },
      "source": [
        "#### Chart - 2  Top 10 Expensive Reaturent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FHW8RdOHOSk"
      },
      "outputs": [],
      "source": [
        "# Chart - 1 visualization code fro Top 10 Expensive Reaturent\n",
        "# Top 10 Expensive Restaurants\n",
        "plt.figure(figsize=(15,6))\n",
        "x = Top_10_Expensive_Restuent['Cost']\n",
        "y = Top_10_Expensive_Restuent['Name']\n",
        "plt.title(\"Top 10 Expensive Restaurant \\n\",fontsize=20,weight='bold')\n",
        "plt.xlabel(\"Cost\",weight='bold',fontsize=15)\n",
        "plt.ylabel(\"Name\",weight='bold',fontsize=15)\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x=x, y=y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFWVQlKGL00l"
      },
      "outputs": [],
      "source": [
        "# Chart - 1 visualization code fro Top 10 Expensive Reaturent\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "x = Top_10_cheapest_resturent['Cost']\n",
        "y = Top_10_cheapest_resturent['Name']\n",
        "plt.title(\"Top 10 Cheapest Restaurant \\n\",fontsize=20,weight='bold')\n",
        "plt.xlabel(\"Cost\",weight='bold',fontsize=15)\n",
        "plt.ylabel(\"Name\",weight='bold',fontsize=15)\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x=x, y=y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QZ13OEpz2H"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XESiWehPqBRc"
      },
      "source": [
        "We have chosen chosen bar plot to find the Expensive and Cheapest resturent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_j1G7yiqdRP"
      },
      "source": [
        "From the given chart we have found top 10 expensive and cheapest resturent from th given dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448CDAPjqfQr"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cspy4FjqxJW"
      },
      "source": [
        "The insigh we get from the given barchart will make high impect in business because it helps us n making the decision financially."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSlN3yHqYklG"
      },
      "source": [
        "#### Chart - 3  :    Top 5 Tagg Resturent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCoknh4kSJyv"
      },
      "outputs": [],
      "source": [
        "# Chart - 2 visualization code pie chart for top 5 Tagging Resturent\n",
        "\n",
        "collection_list = Collections_df.sort_values('Number_of_Restaurants', ascending = False)['Tags'].tolist()[:5]\n",
        "data = Collections_df.sort_values('Number_of_Restaurants', ascending = False) ['Number_of_Restaurants'].tolist()[:5]\n",
        "labels = collection_list\n",
        "\n",
        "\n",
        "#create pie chart\n",
        "plt.pie(data, labels = labels,  autopct='%.0f%%')\n",
        "plt.title('Top 5 Most Tagging Resturent ', size =22, weight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaQc5DSHoTDt"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1zdlAzHoYJc"
      },
      "source": [
        "We have pick up the pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Where differenet percentage comparison comes into action pie chart is used frequently. So, I used Pie chart and which helped me to get the percentage comparision of the variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpmso5SKofVQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwSsjlkOo2wV"
      },
      "source": [
        "From the above chart I got to know that from the top 5 'Tags' there are **28%** of them belong to **'Great Buffets'**, **21%** of them belong to **Food Hygiene Rated Resturents in Hyderabad** , **18%** of them are **Hydrabad's Hottest** and **15%** are **Corporate Favorites**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkD0FXvCrXZW"
      },
      "source": [
        "#### Chart - 4   : WordCloud for Colllections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23yoMy8Y-QIB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Chart - 4 WordCloud for Colllections\n",
        "# Storind all cuisine in form of text\n",
        "plt.figure(figsize=(15,8))\n",
        "text = \" \".join(name for name in Collections_df.Tags )\n",
        "\n",
        "\n",
        "# Creating word_cloud with text as argument in .generate() method\n",
        "\n",
        "word_cloud = WordCloud(width = 1400, height = 1400,collocations = False,\n",
        "                      colormap='rainbow', background_color = 'black').generate(text)\n",
        "\n",
        "# Display the generated Word Cloud\n",
        "\n",
        "\n",
        "plt.title('Reviews \\n',fontsize=20, weight = 'bold')\n",
        "plt.imshow(word_cloud, interpolation='bilinear');\n",
        "\n",
        "plt.axis(\"off\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6dVpIINYklI"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBjSkVoBtrSP"
      },
      "source": [
        "We have pict this WordCloud for the visual representation of a text, in which the words appear bigger the more often they are mentioned. Word clouds are great for visualizing unstructured text data and getting insights on trends and patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmpgYnKYklI"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSx9atu2YklI"
      },
      "source": [
        "From the given Word chart we can clearly say that the word which appear bigger is more offenly used and the more offely used word like Hydrabad, Best, Great Food, Favorities, Rated etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JiQyfWJYklI"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcBbebzrYklV"
      },
      "source": [
        "The given Word chart helps in Advertisement which directly import on business which helps to analyse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM7whBJCYoAo"
      },
      "source": [
        "#### Chart - 5   :    Top 5 most selling Cuisine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htfydzuB3I4_"
      },
      "outputs": [],
      "source": [
        "# Chart - 5 visualization code pie chart for top 5 most selling cuisine\n",
        "cuisine_list = cuisine_df.sort_values('Number_of_Restaurants', ascending = False)['Type_of_Food'].tolist()[:5]\n",
        "data = cuisine_df.sort_values('Number_of_Restaurants', ascending = False)['Number_of_Restaurants'].tolist()[:5]\n",
        "labels = cuisine_list\n",
        "\n",
        "#create pie chart\n",
        "plt.pie(data, labels = labels,  autopct='%.0f%%')\n",
        "plt.title('Top 5 Most Selling type of Foods', size =22, weight ='bold', )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH9YR5n3wv8t"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAWBkYoZwo-J"
      },
      "source": [
        "We have pick up the pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Where differenet percentage comparison comes into action pie chart is used frequently. So, I used Pie chart and which helped me to get the percentage comparision of the variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQUthBs3w_3l"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOq0-QKuw4xV"
      },
      "source": [
        "From the above chart We got to know that important information ie the top **5 Cusine**' there are **39%** of them belong to **North Indian**, **28%** of them belong to **Chinese** , **13%** of them are **Continental** and **10%** are belong to **Biryani** and **10%** are belong to Fast Food."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox-wUq9917EC"
      },
      "source": [
        "#### Chart - 6 Wordcloud for Cuisine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb8nxTd_76Nz"
      },
      "outputs": [],
      "source": [
        "# Chart - 6 wordcloud for Cuisine\n",
        "# storind all cuisine in form of text\n",
        "plt.figure(figsize=(15,8))\n",
        "text = \" \".join(name for name in cuisine_df.Type_of_Food )\n",
        "\n",
        "\n",
        "# Creating word_cloud with text as argument in .generate() method\n",
        "\n",
        "word_cloud = WordCloud(width = 2000, height = 2000, collocations = False, colormap='rainbow', background_color = 'black').generate(text)\n",
        "\n",
        "# Display the generated Word Cloud\n",
        "\n",
        "\n",
        "plt.title('Types of Food \\n',fontsize=20, weight = 'bold')\n",
        "plt.imshow(word_cloud, interpolation='bilinear');\n",
        "\n",
        "plt.axis(\"off\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fge-S5ZAYoAp"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dBItgRVYoAp"
      },
      "source": [
        "We have pict this WordCloud for the visual representation of a text, in which the words appear bigger the more often they are mentioned. Word clouds are great for visualizing unstructured text data and getting insights on trends and patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85gYPyotYoAp"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jstXR6OYoAp"
      },
      "source": [
        "From the given Word chart we can clearly say that the word which appear bigger is more offenly used and the more offely used word like North, Indian, Food, Chinees, Biryani etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoGjAbkUYoAp"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      },
      "source": [
        "The given Word chart helps in Advertisement which directly import on business which helps to analyse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Of9eVA-YrdM"
      },
      "source": [
        "#### Chart - 7  :  Top 10 Reviewers and Their Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OObETdijIm-m"
      },
      "outputs": [],
      "source": [
        "# top reviewers that have more review\n",
        "plt.figure(figsize=(15,6))\n",
        "x = top_10_reviewers['Review_Count']\n",
        "y = top_10_reviewers['Reviewer']\n",
        "plt.title(\"Top 10 Reviewers \\n\",fontsize=20, weight='bold')\n",
        "plt.ylabel(\"Name\",weight='bold',fontsize=15)\n",
        "plt.xlabel(\"No. of Reviews\",weight='bold',fontsize=15)\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x=x, y=y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fmdv3pUPFe7"
      },
      "outputs": [],
      "source": [
        "# Average rating of top reviewers\n",
        "plt.figure(figsize=(15,6))\n",
        "x = top_10_reviewers_ratings['Average_Ratings']\n",
        "y = top_10_reviewers_ratings['Reviewer']\n",
        "plt.title(\"Average Rating of Top 10 reviewers \\n\",fontsize=20, weight='bold')\n",
        "plt.ylabel(\"Name\",weight='bold',fontsize=15)\n",
        "plt.xlabel(\"Average Rating\",weight='bold',fontsize=15)\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x=x, y=y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iky9q4vBYrdO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJRCwT6DYrdO"
      },
      "source": [
        "We have chosen chosen bar plot to find the Reviews and their Average Review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6T5p64dYrdO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      },
      "source": [
        "From the given braplot we have found top 10 Reviewers and their review are importent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Ehk30pYrdP"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLNxxz7MYrdP"
      },
      "source": [
        "The Review of these top Reviewer is important because they have good knowledge about the that things so from thier review company can improve their quality which lead to growth in their business."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bamQiAODYuh1"
      },
      "source": [
        "#### Chart - 8 : Most Followed Reviewer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KLesEwMVF0S"
      },
      "outputs": [],
      "source": [
        "# visualization code for most review follower\n",
        "plt.figure(figsize=(15,6))\n",
        "sns.barplot(data = most_followed_reviewer[:10], x = 'Followers', y = 'Reviewer',palette='bright')\n",
        "plt.title('Most followed Reviewer \\n',fontsize=20, weight = 'bold')\n",
        "plt.ylabel(\"Reviewers Name\",weight='bold',fontsize=15)\n",
        "plt.xlabel(\"Followers\",weight='bold',fontsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz2K9lAH-DU5"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff358TG1-LGv"
      },
      "source": [
        "We have chosen chosen bar plot to find the Reviewers Name and Their Follower"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZmPfKJS-EWK"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDsEjW88-Orf"
      },
      "source": [
        "From the given chart we have found top 10 Reviewer Name on the basis of follower basis from the given dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBT5AiPI8dlZ"
      },
      "source": [
        "#### Chart - 9 :    Word cloud for Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEMqqiP_KfCP"
      },
      "outputs": [],
      "source": [
        "#Creating word cloud for reviews\n",
        "plt.figure(figsize=(15,8))\n",
        "text = \" \".join(name for name in review_df.sort_values('Review',ascending=False).Review[:30])\n",
        "\n",
        "\n",
        "# Creating word_cloud with text as argument in .generate() method\n",
        "\n",
        "word_cloud = WordCloud(width = 1400, height = 1400, collocations = False, background_color = 'black').generate(text)\n",
        "\n",
        "# Display the generated Word Cloud\n",
        "\n",
        "plt.imshow(word_cloud, interpolation='bilinear')\n",
        "\n",
        "plt.title('Given Reviews \\n',fontsize=20, weight = 'bold')\n",
        "plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxuIMRPYuh3"
      },
      "source": [
        "We have pict this WordCloud for the visual representation of a text, in which the words appear bigger the more often they are mentioned. Word clouds are great for visualizing unstructured text data and getting insights on trends and patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzvFGzlYuh3"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyqkiB8YYuh3"
      },
      "source": [
        "From the given Word chart we can clearly say that the word which appear bigger is more offenly used and the more offely used word like Food, Good, Place, Resturent so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpmQ266Yuh3"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      },
      "source": [
        "The given Word chart helps in Advertisement which directly import on business which helps to analyse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC_X3p0fY2L0"
      },
      "source": [
        "#### Chart - 10 - Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "outputs": [],
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(merged_df.corr(numeric_only=True), cmap ='PiYG',annot = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      },
      "source": [
        "We have pick up this heatmap chart to find insights to analyse that how the given one variable are the corelation to another variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfSqtnDqZNRR"
      },
      "source": [
        "From the heat map we can say that no any variable is corelated to any one so the variable of the data which is provided so we can implement the algorithms without dealing with the corelation.\n",
        "\n",
        "\n",
        "\n",
        "*   Pictures and follower are positively correlated with each orther by - 28%\n",
        "\n",
        "*   Pictures and Reviews are possitively correlated with each orther by - 33%\n",
        "\n",
        "*   Year and Cost are possitively correlated with each orther by - 25%\n",
        "\n",
        "*   Followers and Reviews are positively correlated with each orther by - 48%\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q29F0dvdveiT"
      },
      "source": [
        "#### Chart - 11 - Pair Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "outputs": [],
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(merged_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXh0U9oCveiU"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMmPjTByveiU"
      },
      "source": [
        "Pair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters. It also helps to form some simple classification models by drawing some simple lines or make linear separation in our data-set.\n",
        "\n",
        "Thus, I used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22aHeOlLveiV"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPQ8RGwHveiV"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The cost of a restaurant is positively correlated with the rating it receives.\n",
        "\n",
        "* Restaurants that are reviewed by reviewers with more followers will have a higher rating.\n",
        "\n",
        "* Restaurants that offer a wider variety of cuisines will have a higher rating."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cost of a restaurant is positively correlated with the rating it receives."
      ],
      "metadata": {
        "id": "4lbpWkRg2-jx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Null hypothesis: There is no relationship between the cost of restaurant and the rating it receives. (H0: 1 = 0)\n",
        "* Alternative hypothesis: There is a positive relationship between the cost of a restaurant and the rating it receives. (H1: 1 > 0)\n",
        "* Test : Simple Linear Regression Analysis"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "\n",
        "# fit the linear model\n",
        "model = smf.ols(formula='Rating ~ Cost', data= merged_df).fit()\n",
        "\n",
        "# Check p-value of coefficient\n",
        "p_value = model.pvalues[1]\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject Null Hypothesis - There is no relationship between the cost of\\\n",
        " restaurant and the rating it receives.\")\n",
        "else:\n",
        "    print(\"Fail to reject Null Hypothesis - There is a positive relationship \\\n",
        " between the cost of a restaurant and the rating it receives.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Linear regression test for checking the relationship between the cost of a restaurant and its rating"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this test because it is a common and straight forward method for testing the relationship between two continuous variables. This would involve fitting a linear model with the rating as the dependent variable and the cost as the independent variable. The p-value of the coefficient for the cost variable can then be used to determine if there is a statistically significant relationship between the two variables."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restaurants that are reviewed by reviewers with more followers will have a higher rating."
      ],
      "metadata": {
        "id": "dym0pGw84hHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Null hypothesis: The number of followers a reviewer has has no effect on the rating of a restaurant. (H0: 1 = 0)\n",
        "* Alternative hypothesis: Alternative Hypothesis: The number of followers a reviewer has has a positive effect on the rating of a restaurant. (H1: 1 > 0)\n",
        "* Test : Simple Linear Regression test"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import statsmodels.formula.api as smf\n",
        "# fit the linear model\n",
        "model = smf.ols(formula='Rating ~ Followers', data = merged_df).fit()\n",
        "# print the summary of the model\n",
        "# print(model.summary())\n",
        "# extract p-value of coefficient for Reviewer_Followers\n",
        "p_value = model.pvalues[1]\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject Null Hypothesis : That's mean The number of followers a reviewer has has no effect on the rating of a restaurant.\")\n",
        "else:\n",
        "    print(\"Fail to reject Null Hypothesis : That's Mean The number of followers a reviewer has has a positive effect on the rating of a restaurant.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the second hypothesis, I have used Simple Linear Regression Test."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I choose this test because it is a straightforward method for testing the relationship between two continuous variables. It assumes that there is a linear relationship between the independent variable (Reviewer_Followers) and the dependent variable (Rating) and it allows us to estimate the strength and direction of that relationship. It also allows us to test the null hypothesis that there is no relationship between the two variables by testing the p-value of the coefficient of the independent variable."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restaurants that offer a wider variety of cuisines will have a higher rating."
      ],
      "metadata": {
        "id": "4vbUo4RS5SbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Null hypothesis: The variety of cuisines offered by a restaurant has no effect on its rating. (H0: 3 = 0)\n",
        "* Alternative hypothesis: The variety of cuisines offered by a restaurant has a positive effect on its rating. (H1: 3 > 0)\n",
        "* Test : Chi-Squared Test"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the column names in your DataFrame\n",
        "print(merged_df.columns)\n",
        "\n",
        "# Replace 'Cuisines' with the actual column name\n",
        "# For example if the column name is 'Cuisine'\n",
        "pd.crosstab(merged_df['Cuisines'], merged_df['Rating'])[:1]"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# create a contingency table\n",
        "ct = pd.crosstab(merged_df['Cuisines'], merged_df['Rating'])\n",
        "\n",
        "# perform chi-squared test\n",
        "chi2, p, dof, expected = chi2_contingency(ct)\n",
        "\n",
        "# Check p-value\n",
        "if p < 0.05:\n",
        "    print(\"Reject Null Hypothesis : Thats's mean the variety of cuisines offered by a restaurant has no effect on its rating.\")\n",
        "else:\n",
        "    print(\"Fail to reject Null Hypothesis : That's mean the variety of cuisines offered by a restaurant has a positive effect on its rating.\")"
      ],
      "metadata": {
        "id": "o6V91cXJ6QUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the third hypothesis, I have used chi-squared test for independence to test the relationship between the variety of cuisines offered by a restaurant and its rating."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I choose this test because it is suitable for comparing the relationship between two categorical variables. This would involve creating a contingency table with the number of restaurants that offer each cuisine as the rows and the rating of the restaurant as the columns."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjJCtPM0KBk"
      },
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyOF9F70UgQ"
      },
      "source": [
        "### 1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAqh1gdj_OEM"
      },
      "source": [
        "Heandle Duplicate Values from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJIzPmBm_XeO"
      },
      "outputs": [],
      "source": [
        "# Check the duplcate values persent in the data set\n",
        "print(f'Total number of Duplicate Value persent in Restuent datast set {resturent_df.duplicated().sum()} \\n')\n",
        "print(f'Total number of Duplicate Value persent in Review datast set {review_df.duplicated().sum()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54P0nl42AA8n"
      },
      "outputs": [],
      "source": [
        "# Drop Duplicate values from the dataset\n",
        "\n",
        "review_df.drop_duplicates(inplace= True)\n",
        "\n",
        "# check the duplcate value after drop\n",
        "\n",
        "print(f'Total numer of Duplicate Value persent in Restuent datast set {resturent_df.duplicated().sum()} \\n')\n",
        "print(f'Total numer of Duplicate Value persent in Review datast set {review_df.duplicated().sum()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIy3bQ5Taacz"
      },
      "source": [
        "### Handle Missing values from Resturent Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "outputs": [],
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "resturent_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVfWMJofasnn"
      },
      "outputs": [],
      "source": [
        "# Fill null values with mode\n",
        "resturent_df.Timings.fillna(resturent_df.Timings.mode()[0],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rNh2nsHbk2y"
      },
      "outputs": [],
      "source": [
        "# check percentage of missing values  in Collections\n",
        "\n",
        "missing_percentage = ((resturent_df['Collections'].isnull().sum())/(len(resturent_df['Collections'])))*100\n",
        "print(f'Percentage of missing value in Collections is {round(missing_percentage, 2)}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6Iru3JPcAok"
      },
      "outputs": [],
      "source": [
        "# More then 50% of the data is missing so we are droping the colllection collumns\n",
        "resturent_df.drop('Collections', axis=1, inplace = True)\n",
        "resturent_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_kd8BlIcudM"
      },
      "source": [
        "### Handle Missing values from Resturent Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaMygJikcqIe"
      },
      "outputs": [],
      "source": [
        "review_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRgUebPWgiY3"
      },
      "outputs": [],
      "source": [
        "review_df[review_df['Reviewer'].isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY8IQV81hNMy"
      },
      "outputs": [],
      "source": [
        "review_df['Restaurant'].dropna(inplace = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cydK5KRTiqo5"
      },
      "outputs": [],
      "source": [
        "review_df = review_df.dropna(subset=['Restaurant','Reviewer','Reviews'])\n",
        "review_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IboDxBxjjuCJ"
      },
      "outputs": [],
      "source": [
        "#filling null values in review and reviewer follower column\n",
        "review_df = review_df.fillna({\"Review\": \"No Review\", \"Followers\": 0})\n",
        "review_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVGkyTBqkRTG"
      },
      "outputs": [],
      "source": [
        "# Merge Both Dataset\n",
        "merge_df= resturent_df.merge(review_df, on = 'Restaurant')\n",
        "merge_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wuGOrhz0itI"
      },
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ixusLtI0pqI"
      },
      "source": [
        "We started with removing duplicte values persent in review dataset and handle missing vlaues persent in both resturent and Review dataset.\n",
        "\n",
        "\n",
        "Dataset that contains details about Resturent, had 1 null value in timing feature and more than 50% null value in collection feature. In order to treat with those I first replaced the null value for timing with mode since there was only one null and mode is robust to outliers plus that hotel name was one unique feature which had all other feature except timing and collection so it was better to preserve that data. Since there was more than 50% null values in collection feature, I removed the entire column because columns with a high percentage of null values are likely to have a lot of missing data, which can make it difficult to accurately analyze or make predictions based on the data.\n",
        "\n",
        "In the dataset tha has details of reviewer had Reviewer - 2, Review - 9, Rating - 2, Metadata - 2, Time - 2, Reviewer_Total_Review- 3, Reviewer_Followers - 1581, Review_Year - 2, Review_Month - 2, Review_Hour - 2. On analysing I found that feature like reviewer and reviewer total review had all null values, therefore I removed those two columns which made null values in other feature to zero except in review and reviewer followers columns. Since review was textual data, I changed those 7 null values to 'no review' and reviewer followers to 0 as follower is the meta data for reviewer and it can be 0.\n",
        "\n",
        "And thus all the null values were treated, at the end I then again merged both the dataset hotel and review dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1riN9m0vUs"
      },
      "source": [
        "### 2. Handling Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTCaYh9QhyUi"
      },
      "source": [
        "#### Anamoly Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "outputs": [],
      "source": [
        "#Anamoly detection\n",
        "from sklearn.ensemble import IsolationForest\n",
        "#checking for normal distribution\n",
        "print(\"Skewness - Cost: %f\" % merge_df['Cost'].skew())\n",
        "print(\"Kurtosis - Cost: %f\" % merge_df['Cost'].kurt())\n",
        "print(\"Skewness - Followers: %f\" % merge_df['Followers'].skew())\n",
        "print(\"Kurtosis - Followers: %f\" % merge_df['Followers'].kurt())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0DDe80NeHre"
      },
      "outputs": [],
      "source": [
        "# Scatter Plot of Cost\n",
        "plt.scatter(range(merge_df.shape[0]), np.sort(merge_df['Cost'].values))\n",
        "plt.xlabel('index')\n",
        "plt.ylabel('Cost')\n",
        "plt.title(\"Cost distribution\")\n",
        "sns.despine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjpWKAJdeT-x"
      },
      "outputs": [],
      "source": [
        "#distribution of cost\n",
        "sns.distplot(merge_df['Cost'])\n",
        "plt.title(\"Distribution of Cost\")\n",
        "sns.despine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8qZpTqkfJa-"
      },
      "outputs": [],
      "source": [
        "#plot for reviewer follower\n",
        "plt.scatter(range(merge_df.shape[0]), np.sort(merge_df['Followers'].values))\n",
        "plt.xlabel('index')\n",
        "plt.ylabel('Followers')\n",
        "plt.title(\"Followers distribution\")\n",
        "sns.despine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzQibjz0fXbT"
      },
      "outputs": [],
      "source": [
        "#distribution of Reviewer_Followers\n",
        "sns.distplot(merge_df['Followers'])\n",
        "plt.title(\"Distribution of Followers\")\n",
        "sns.despine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFsSZPSNgFgk"
      },
      "outputs": [],
      "source": [
        "#isolation forest for anamoly detection on cost\n",
        "isolation_forest = IsolationForest(n_estimators=100, contamination=0.01)\n",
        "isolation_forest.fit(merge_df['Cost'].values.reshape(-1, 1))\n",
        "merge_df['anomaly_score_univariate_Cost'] = isolation_forest.decision_function(merge_df['Cost'].values.reshape(-1, 1))\n",
        "merge_df['outlier_univariate_Cost'] = isolation_forest.predict(merge_df['Cost'].values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5H7VvligXlU"
      },
      "outputs": [],
      "source": [
        "#chart to visualize outliers\n",
        "xx = np.linspace(merge_df['Cost'].min(), merge_df['Cost'].max(), len(merge_df)).reshape(-1,1)\n",
        "anomaly_score = isolation_forest.decision_function(xx)\n",
        "outlier = isolation_forest.predict(xx)\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(xx, anomaly_score, label='anomaly score')\n",
        "plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score),\n",
        "where=outlier==-1, color='r',\n",
        "alpha=.4, label='outlier region')\n",
        "plt.legend()\n",
        "plt.ylabel('anomaly score')\n",
        "plt.xlabel('Cost')\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPZnZBJ2gqWD"
      },
      "outputs": [],
      "source": [
        "#isolation forest for anamoly detection of reviewer follower\n",
        "isolation_forest = IsolationForest(n_estimators=100, contamination=0.01)\n",
        "isolation_forest.fit(merge_df['Followers'].values.reshape(-1, 1))\n",
        "merge_df['anomaly_score_univariate_follower'] = isolation_forest.decision_function(\n",
        "    merge_df['Followers'].values.reshape(-1, 1))\n",
        "merge_df['outlier_univariate_follower'] = isolation_forest.predict(\n",
        "    merge_df['Followers'].values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DycE7QrhCmK"
      },
      "outputs": [],
      "source": [
        "#chat to visualize outliers in reviwer follower column\n",
        "xx = np.linspace(merge_df['Followers'].min(), merge_df['Followers'].max(), len(merge_df)).reshape(-1,1)\n",
        "anomaly_score = isolation_forest.decision_function(xx)\n",
        "outlier = isolation_forest.predict(xx)\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(xx, anomaly_score, label='anomaly score')\n",
        "plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score),\n",
        "where=outlier==-1, color='r',\n",
        "alpha=.4, label='outlier region')\n",
        "plt.legend()\n",
        "plt.ylabel('anomaly score')\n",
        "plt.xlabel('Reviewer_Followers')\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxAf5BqhiF_N"
      },
      "source": [
        "####Treating Outlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jaLFbAKiE0h"
      },
      "outputs": [],
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# To separate the symmetric distributed features and skew symmetric distributed features\n",
        "symmetric_feature=[]\n",
        "non_symmetric_feature=[]\n",
        "for i in merge_df.describe().columns:\n",
        "  if abs(merge_df[i].mean()-merge_df[i].median())<0.2:\n",
        "    symmetric_feature.append(i)\n",
        "  else:\n",
        "    non_symmetric_feature.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\",symmetric_feature)\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\",non_symmetric_feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ASxaGfNyFoz"
      },
      "outputs": [],
      "source": [
        "# For Skew Symmetric features defining upper and lower boundry\n",
        "#Outer Fence\n",
        "def outlier_treatment_skew(df,feature):\n",
        "  IQR= df[feature].quantile(0.75)- df[feature].quantile(0.25)\n",
        "  lower_bridge =df[feature].quantile(0.25)- 1.5*IQR\n",
        "  upper_bridge =df[feature].quantile(0.75)+ 1.5*IQR\n",
        "  # print(f'upper : {upper_bridge} lower : {lower_bridge}')\n",
        "  return upper_bridge,lower_bridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQWm_ZXmyLS8"
      },
      "outputs": [],
      "source": [
        "# Restricting the data to lower and upper boundary for cost in hotel dataset\n",
        "#lower limit capping\n",
        "resturent_df.loc[resturent_df['Cost']<= outlier_treatment_skew(df=resturent_df, feature='Cost')[1], 'Cost']=outlier_treatment_skew(df=resturent_df,feature='Cost')[1]\n",
        "\n",
        "#upper limit capping\n",
        "resturent_df.loc[resturent_df['Cost']>= outlier_treatment_skew(df=resturent_df, feature='Cost')[0], 'Cost']=outlier_treatment_skew(df=resturent_df,feature='Cost')[0]\n",
        "\n",
        "\n",
        "# Restricting the data to lower and upper boundary for Reviewer followers in review dataset\n",
        "#lower limit capping\n",
        "review_df.loc[review_df['Followers']<= outlier_treatment_skew(df=review_df,feature='Followers')[1], 'Followers']=outlier_treatment_skew(df=review_df,feature='Followers')[1]\n",
        "\n",
        "#upper limit capping\n",
        "review_df.loc[review_df['Followers']>= outlier_treatment_skew(df=review_df, feature='Followers')[0], 'Followers']=outlier_treatment_skew(df=review_df,feature='Followers')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR8kjIp9z-YQ"
      },
      "outputs": [],
      "source": [
        "#dropping the columns created while outliers treatment\n",
        "merge_df.drop(columns =['anomaly_score_univariate_Cost','outlier_univariate_Cost','anomaly_score_univariate_follower','outlier_univariate_follower'], inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70OUqCRg7tSg"
      },
      "outputs": [],
      "source": [
        "#adding average rating - will remove 5 unrated restaurant from 105 restaurant\n",
        "avg_hotel_rating.rename(columns = {'Rating':'Average_Rating'}, inplace =True)\n",
        "resturent_df = resturent_df.merge(avg_hotel_rating[['Average_Rating','Restaurant']], on = 'Restaurant')\n",
        "resturent_df.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578E2V7j08f6"
      },
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGZz5OrT1HH-"
      },
      "source": [
        "We have found that follower feature show positively skew distribution and using isolation forest they have outliers, So We have used capping technique instead of removing outliers, capped outliers with the highest and lowest limit using IQR method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xtkJwZ18nB"
      },
      "source": [
        "### 3. Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnwAoYDs2r1a"
      },
      "outputs": [],
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "#categorial encoding using pd.getdummies\n",
        "#new df with important categories\n",
        "cluster_dummy = resturent_df[['Restaurant','Cuisines']]\n",
        "\n",
        "#spliting cuisines as they are separted with comma and converting into list\n",
        "cluster_dummy['Cuisines'] = cluster_dummy['Cuisines'].str.split(',')\n",
        "\n",
        "#using explode converting list to unique individual items\n",
        "cluster_dummy = cluster_dummy.explode('Cuisines')\n",
        "\n",
        "#removing extra trailing space from cuisines after exploded\n",
        "cluster_dummy['Cuisines'] = cluster_dummy['Cuisines'].apply(lambda x: x.strip())\n",
        "\n",
        "#using get dummies to get dummies for cuisines\n",
        "cluster_dummy = pd.get_dummies(cluster_dummy, columns=[\"Cuisines\"], prefix=[\"Cuisines\"])\n",
        "\n",
        "#checking if the values are correct\n",
        "cluster_dummy.loc[:, cluster_dummy.columns.str.startswith('Cuisines_')].idxmax(1)[:6]\n",
        "\n",
        "#replacing cuisines_ from columns name - for better understanding run seperatly\n",
        "cluster_dummy.columns = cluster_dummy.columns.str.replace(\"Cuisines_\",\"\")\n",
        "\n",
        "\n",
        "#grouping each restaurant as explode created unnecessary rows\n",
        "cluster_dummy = cluster_dummy.groupby(\"Restaurant\").sum().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsPLQmlK3brH"
      },
      "outputs": [],
      "source": [
        "#total cuisine count\n",
        "resturent_df['Total_Cuisine_Count'] = resturent_df['Cuisines'].apply(lambda x : len(x.split(',')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnEVJAFoILe9"
      },
      "outputs": [],
      "source": [
        "resturent_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo9myu7OOu74"
      },
      "outputs": [],
      "source": [
        "cluster_dummy.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZCmlHUk9f9P"
      },
      "outputs": [],
      "source": [
        "#adding cost column to the new dataset\n",
        "cluster_dummy = resturent_df[['Restaurant','Cost','Average_Rating','Total_Cuisine_Count']].merge(cluster_dummy, on = 'Restaurant')\n",
        "cluster_dummy.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXOUHXQe_RS2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Alternate Method for creating dummies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdrio6Pt_DfJ"
      },
      "outputs": [],
      "source": [
        "#creating data frame for categorial encoding\n",
        "cluster_df = resturent_df[['Restaurant','Cuisines','Cost','Average_Rating','Total_Cuisine_Count']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzEti3HA_fCz"
      },
      "outputs": [],
      "source": [
        "#creating new dataframe for clustering\n",
        "cluster_df = pd.concat([cluster_df,pd.DataFrame(columns=list(cuisine_dict.keys()))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUw1Sa7y_pOD"
      },
      "outputs": [],
      "source": [
        "#creating categorial feature for cuisine\n",
        "#iterate over every row in the dataframe\n",
        "for i, row in cluster_df.iterrows():\n",
        "  # iterate over the new columns\n",
        "  for column in list(cluster_df.columns):\n",
        "      if column not in ['Restaurant','Cost','Cuisines','Average_Rating','Total_Cuisine_Count']:\n",
        "        # checking if the column is in the list of cuisines available for that row\n",
        "        if column in row['Cuisines']:\n",
        "          #assign it as 1 else 0\n",
        "          cluster_df.loc[i,column] = 1\n",
        "        else:\n",
        "          cluster_df.loc[i,column] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N07TYbPa_1LU"
      },
      "outputs": [],
      "source": [
        "#result from encoding\n",
        "cluster_df.head(2).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67NQN5KX2AMe"
      },
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDaue5h32n_G"
      },
      "source": [
        "We have used one hot encoding on the cuisine category and based on the cuisine if present i gave value to 1 and if absent gave value of 0.\n",
        "Benefit of using one hot encoding:\n",
        "\n",
        "* Handling categorical variables with no ordinal relationship:\n",
        "> One-hot encoding does not assume any ordinal relationship between the categories, making it suitable for categorical features that do not have a natural ordering.\n",
        "\n",
        "* Handling categorical variables with many unique values\n",
        "> One-hot encoding can handle categorical features with a high cardinality, which can be useful when there are many unique categories.\n",
        "\n",
        "* Handling categorical variables with multiple levels\n",
        "> One-hot encoding can handle categorical features with multiple levels, such as \"state\" and \"city\". This can be useful when there are many unique combinations of levels.\n",
        "\n",
        "* Handling categorical variables with missing values\n",
        "> One-hot encoding can handle missing values by creating a new category for them.\n",
        "\n",
        "* Model interpretability\n",
        "> One-hot encoded features are easy to interpret as the encoded values are binary, thus making it easy to understand the relationship between the categorical feature and the target variable.\n",
        "\n",
        "* Compatibility with many machine learning models\n",
        "> One-hot encoded features are compatible with most machine learning models, including linear and logistic regression, decision trees, and neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwf50b-R2tYG"
      },
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMQiZwjn3iu7"
      },
      "source": [
        "#### 1. Expand Contraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "outputs": [],
      "source": [
        "# creating new df for text processing of sentiment analysis\n",
        "sentiment_df = review_df[['Reviewer','Restaurant','Rating','Review']]\n",
        "# analysing five random sample\n",
        "sentiment_df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdEqhiLeWMGS"
      },
      "outputs": [],
      "source": [
        "#setting index\n",
        "sentiment_df = sentiment_df.reset_index()\n",
        "sentiment_df['index'] = sentiment_df.index\n",
        "\n",
        "\n",
        "sentiment_df.sample(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FZQSIkfbLJr"
      },
      "outputs": [],
      "source": [
        " # Install contractions\n",
        "!pip install contractions\n",
        "\n",
        "# import sys\n",
        "import sys\n",
        "!{sys.executable} -m pip install contractions\n",
        "\n",
        "# import contactions\n",
        "import contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLMl8-sCWf1T"
      },
      "outputs": [],
      "source": [
        "# Expand Contraction\n",
        "\n",
        "# applying fuction for contracting text\n",
        "sentiment_df['Review']=sentiment_df['Review'].apply(lambda x:contractions.fix(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVIkgGqN3qsr"
      },
      "source": [
        "#### 2. Lower Casing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "outputs": [],
      "source": [
        "# Lower Casing\n",
        "sentiment_df['Review'] = sentiment_df['Review'].str.lower()\n",
        "sentiment_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkPnILGE3zoT"
      },
      "source": [
        "#### 3. Removing Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq0ItX9xiy8n"
      },
      "outputs": [],
      "source": [
        "# Code to Remove Punctuations\n",
        "import string\n",
        "def remove_punctuation(text):\n",
        "  '''a function for removing punctuation'''\n",
        "\n",
        "  # replacing the punctuations with no space,\n",
        "  # which in effect deletes the punctuation marks\n",
        "  translator = str.maketrans('', '', string.punctuation)\n",
        "  # return the text stripped of punctuation marks\n",
        "  return text.translate(translator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "outputs": [],
      "source": [
        "#remove punctuation using  Created function\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(remove_punctuation)\n",
        "sentiment_df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlsf0x5436Go"
      },
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3XpDqvYk1cH"
      },
      "outputs": [],
      "source": [
        "# Import Library\n",
        "import re\n",
        "\n",
        "# Remove links\n",
        "sentiment_df[\"Review\"] = sentiment_df[\"Review\"].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
        "\n",
        "# Remove digits\n",
        "sentiment_df[\"Review\"] = sentiment_df[\"Review\"].apply(lambda x: re.sub(r\"\\d+\", \"\", x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfD5ZLsilhHa"
      },
      "outputs": [],
      "source": [
        "#function to extract location of the restaurant\n",
        "def get_location(link):\n",
        "  link_elements = link.split(\"/\")\n",
        "  return link_elements[3]\n",
        "\n",
        "#create a location feature\n",
        "resturent_df['Location'] = resturent_df['Links'].apply(get_location)\n",
        "resturent_df.sample(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT9DMSJo4nBL"
      },
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKQdITm5nIjk"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY3RXHacmxaH"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PT-J-8o1l7eD"
      },
      "outputs": [],
      "source": [
        "# Remove Stopwords\n",
        "# extracting the stopwords from nltk library\n",
        "sw = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "outputs": [],
      "source": [
        "# function call to remove Stopwords\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  '''a function for removing the stopword'''\n",
        "  # removing the stop words and lowercasing the selected words\n",
        "  text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "  # joining the list of words with space separator\n",
        "  return \" \".join(text)\n",
        "\n",
        "# Remove Stopwords\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "outputs": [],
      "source": [
        "# Remove White spaces\n",
        "sentiment_df['Review'] =sentiment_df['Review'].apply(lambda x: \" \".join(x.split()))\n",
        "\n",
        "#random sample\n",
        "sentiment_df.sample(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49ITxTc407N"
      },
      "source": [
        "#### 6. Rephrase Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa9YOwDiGBWJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "#importing stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX9jFxhLBdZq"
      },
      "outputs": [],
      "source": [
        "# # Rephrase Text\n",
        "\n",
        "\n",
        "# function to create rephrase sentence\n",
        "def rephrase_sentence(sentence):\n",
        "     # Tokenize the sentence\n",
        "     tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "     # Replace each token with its synonyms\n",
        "     new_sentence = []\n",
        "     for token in tokens:\n",
        "         synonyms = wordnet.synsets(token)\n",
        "         if synonyms:\n",
        "             new_sentence.append(synonyms[0].lemmas()[0].name())\n",
        "         else:\n",
        "             new_sentence.append(token)\n",
        "\n",
        "      #Join the tokens back into a sentence\n",
        "     rephrased_sentence = \" \".join(new_sentence)\n",
        "\n",
        "     return rephrased_sentence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "outputs": [],
      "source": [
        " #apply the function to the 'Review' column\n",
        " sentiment_df['Review'] = sentiment_df['Review'].apply(rephrase_sentence)\n",
        " sentiment_df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeJFEK0N496M"
      },
      "source": [
        "#### 7. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(nltk.word_tokenize)\n",
        "sentiment_df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ExmJH0g5HBk"
      },
      "source": [
        "#### 8. Text Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMGLbeQLHoNB"
      },
      "outputs": [],
      "source": [
        "#applying Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create a lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Lemmatize the 'Review' column\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(lemmatize_tokens)\n",
        "\n",
        "sentiment_df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJNqERVU536h"
      },
      "source": [
        "##### Which text normalization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9jKVxE06BC1"
      },
      "source": [
        "We have used **Lemmatization** as a text normalization technique.\n",
        "\n",
        "Lemmatization is the process of reducing words to their base or root form, similar to stemming. However, lemmatization uses a dictionary-based approach and considers the context of the word in order to determine its base form, while stemming uses simple heuristics and does not consider the context of the word. Lemmatization is a more accurate way of finding the root form of a word as it takes into account the context of the word as well as its grammatical structure.\n",
        "\n",
        "I have used lemmatization because it is a more accurate way of reducing words to their base form than stemming. Lemmatization considers the context of the word and its grammatical structure to determine its base form, which can help to improve the performance of natural language processing models. Lemmatization is often used in tasks such as text classification and information retrieval, where the meaning of the words is important.\n",
        "\n",
        "---\n",
        "\n",
        "**Other Method for Normalization**\n",
        "\n",
        "---\n",
        "Tokenization is the process of breaking down a sentence or a piece of text into individual words or tokens. Tokenization is an important step in natural language processing as it allows us to work with individual words rather than the entire text.\n",
        "\n",
        "Stemming is the process of reducing words to their base or root form. This is useful in natural language processing because it allows us to reduce the dimensionality of the data by converting words to their common form. This can help improve the performance of models by reducing the number of unique words that need to be processed.\n",
        "\n",
        "Stemming can be used because they are common normalization techniques used in natural language processing to preprocess text data before it is fed into a model. Tokenization is the first step and it breaks down the text into individual words, which is necessary for most NLP tasks. Stemming is used to reduce the dimensionality of the data by converting words to their common form, this is useful for text classification and other NLP tasks where the meaning of the words is important.\n",
        "\n",
        "In general, stemming is a more aggressive technique that can remove more of the original word form, which may make it difficult for a lemmatizer to accurately identify the base form of the word. Additionally, some stemming algorithms may create non-real words, which are difficult for a lemmatizer to handle.\n",
        "Therefore, if the goal is to maintain the meaning of the text and preserve more of the original word forms, it would be more appropriate to apply lemmatization before stemming. However, if the goal is to reduce all words to their base forms and to group together different forms of the same word, it may be useful to try both ways and compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5UmGsbsOxih"
      },
      "source": [
        "#### 9. Part of speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "outputs": [],
      "source": [
        "# POS Taging\n",
        "sentiment_tfid = sentiment_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VpRboPlJXlw"
      },
      "outputs": [],
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWG55KhjJETP"
      },
      "outputs": [],
      "source": [
        "sentiment_tfid['Review'] = sentiment_tfid['Review'].apply(nltk.pos_tag)\n",
        "sentiment_tfid.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phTlL6xRkwQn"
      },
      "source": [
        "Part-of-speech (POS) tagging can be important for sentiment analysis in some cases, as it can provide additional information about the structure and meaning of the text.\n",
        "\n",
        "For example, certain POS tags, such as adjectives and adverbs, are often used to express sentiment. By identifying these POS tags in the text, a sentiment analysis model can gain a better understanding of the sentiment being expressed. Additionally, certain grammatical structures, such as negations or modals, can change the sentiment of a sentence. By identifying these structures through POS tagging, a sentiment analysis model can take them into account when determining the overall sentiment of the text.\n",
        "\n",
        "However, it's worth noting that POS tagging is not always necessary for sentiment analysis. In some cases, a model may be able to achieve good performance without using POS tagging. Additionally, the complexity of a model that uses POS tagging increases, which could lead to longer training time and higher computational cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      },
      "source": [
        "#### 10. Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "outputs": [],
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
        "vectorizer.fit(sentiment_df['Review'].values)\n",
        "\n",
        "#creating independent variable for sentiment analysis\n",
        "X_tfidf = vectorizer.transform(sentiment_df['Review'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmdhKnv-Mity"
      },
      "outputs": [],
      "source": [
        "!pip install gensim\n",
        "import gensim\n",
        "from gensim import corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwOwkuzfLVdj"
      },
      "outputs": [],
      "source": [
        "#Bag of Words\n",
        "tokenized_text = []\n",
        "for token in sentiment_df['Review']:\n",
        "    tokenized_text.append(token)\n",
        "\n",
        "#creating token dict\n",
        "tokens_dict = gensim.corpora.Dictionary(tokenized_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QVmbpn2Mrzr"
      },
      "outputs": [],
      "source": [
        "#print token dict\n",
        "print(tokens_dict.token2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzy3E0OmNjIl"
      },
      "outputs": [],
      "source": [
        "#using tokens_dict.doc2bow() to generate BoW features for each tokenized course\n",
        "texts_bow = [tokens_dict.doc2bow(text) for text in tokenized_text]\n",
        "\n",
        "#creating a new text_bow dataframe based on the extracted BoW features\n",
        "tokens = []\n",
        "bow_values = []\n",
        "doc_indices = []\n",
        "doc_ids = []\n",
        "for text_idx, text_bow in enumerate(texts_bow):\n",
        "    for token_index, token_bow in text_bow:\n",
        "        token = tokens_dict.get(token_index)\n",
        "        tokens.append(token)\n",
        "        bow_values.append(token_bow)\n",
        "        doc_indices.append(text_idx)\n",
        "        doc_ids.append(sentiment_df[\"Restaurant\"][text_idx])\n",
        "\n",
        "bow_dict = {\"doc_index\": doc_indices,\n",
        "            \"doc_id\": doc_ids,\n",
        "            \"token\": tokens,\n",
        "            \"bow\": bow_values,\n",
        "            }\n",
        "bows_df = pd.DataFrame(bow_dict)\n",
        "bows_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMux9mC6MCf"
      },
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su2EnbCh6UKQ"
      },
      "source": [
        "Here I have used Tf-idf Vectorization technique.\n",
        "\n",
        "TF-IDF (term frequency-inverse document frequency) is a technique that assigns a weight to each word in a document. It is calculated as the product of the term frequency (tf) and the inverse document frequency (idf).\n",
        "\n",
        "The term frequency (tf) is the number of times a word appears in a document, while the inverse document frequency (idf) is a measure of how rare a word is across all documents in a collection. The intuition behind tf-idf is that words that appear frequently in a document but not in many documents across the collection are more informative and thus should be given more weight.\n",
        "\n",
        "The mathematical formula for tf-idf is as follows:\n",
        "\n",
        "tf-idf(t, d, D) = tf(t, d) * idf(t, D)\n",
        "\n",
        "where t is a term (word), d is a document, D is a collection of documents, tf(t, d) is the term frequency of t in d, and idf(t, D) is the inverse document frequency of t in D.\n",
        "\n",
        "The tf component of the weight assigns a value to a word based on how often it appears in the document, while the idf component assigns a value based on how rare the word is in the entire collection of documents. Tf-idf is commonly used in text classification and information retrieval tasks because it can help to down-weight the effect of common words and up-weight the effect of rare words which are more informative.\n",
        "\n",
        "It also helps to reduce the dimensionality of the data and increases the weight of important words, thus providing more informative and robust feature set for the model to work on.\n",
        "\n",
        "Text vectorization is the process of converting text data into numerical vectors that can be used as input for machine learning models.\n",
        "\n",
        "There are several ways to vectorize text data, one of the most common methods is using Tf-idf Vectorization, other methods are bag-of-words (BoW - uses CountVectorizer), word2vec, or doc2vec model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      },
      "source": [
        "### 4. Feature Manipulation & Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74aWNz2AliB"
      },
      "source": [
        "#### 1. Feature Manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxeb8ttwO7fN"
      },
      "source": [
        "#### Resturent Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddHqbt7xPCM7"
      },
      "outputs": [],
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "print(f' The number of row and columns persent in Resturent Dataset : {resturent_df.shape} , \\n ')\n",
        "print('--'*50, '\\n')\n",
        "\n",
        "print('All the variable persent in Resturent Dataset \\n')\n",
        "print(list(resturent_df.columns),  '\\n \\n')\n",
        "\n",
        "\n",
        "resturent_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7esYS0iQPtA5"
      },
      "outputs": [],
      "source": [
        "cluster_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGfKVCNIPx3D"
      },
      "outputs": [],
      "source": [
        "cluster_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-4eZHn_Wu72"
      },
      "outputs": [],
      "source": [
        "\n",
        "#dropping cuisine and restaurant from cluster_df\n",
        "cluster_df.drop(columns = ['Restaurant','Cuisines'], axis = 1, inplace= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPlAJZ2DW9MR"
      },
      "outputs": [],
      "source": [
        "cluster_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#alternatively using other variable created earlier during categorial creation\n",
        "cluster_dummy.shape"
      ],
      "metadata": {
        "id": "vO40ksK5LDV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yfQNR4PXdli"
      },
      "source": [
        "#### Review Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "outputs": [],
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "print(f' The number of row and columns persent in Resturent Dataset : {review_df.shape} , \\n ')\n",
        "print('--'*50, '\\n')\n",
        "\n",
        "print('All the variable persent in Resturent Dataset \\n')\n",
        "print(list(review_df.columns),  '\\n \\n')\n",
        "\n",
        "\n",
        "review_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C9nw1cA_7xq"
      },
      "outputs": [],
      "source": [
        "#creating new binary feature called sentiment based on rating which has 1 = positive and 0 = negative\n",
        "sentiment_df['Sentiment'] = sentiment_df['Rating'].apply(lambda x: 1 if x >=sentiment_df['Rating'].mean() else 0)\n",
        "sentiment_df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DejudWSA-a0"
      },
      "source": [
        "#### 2. Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp1tyTLtA0Vi"
      },
      "outputs": [],
      "source": [
        "# All the column persent in Resturent Dataset\n",
        "resturent_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R134C2bBRiS"
      },
      "outputs": [],
      "source": [
        "# All the column persent in cluster dummy Dataset\n",
        "cluster_dummy.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "061FbNtTBf6X"
      },
      "outputs": [],
      "source": [
        "# All the column persent in review Dataset\n",
        "review_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGqOn6lsBr8D"
      },
      "outputs": [],
      "source": [
        "# Feature Selected for clustering\n",
        "cluster_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Zvb5XS1Blqd"
      },
      "outputs": [],
      "source": [
        "#feature selected for sentiment analysis\n",
        "sentiment_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEMng2IbBLp7"
      },
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      },
      "source": [
        "We had used **PCA for feature selection**, which will be again beneficial for dimensional reduction, therefore will do the needfull in the precedding step.\n",
        "\n",
        "The goal of PCA is to identify the most important variables or features that capture the most variation in the data, and then to project the data onto a lower-dimensional space while preserving as much of the variance as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      },
      "source": [
        "##### Which all features you found important and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGgaEstsBnaf"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNVZ9zx19K6k"
      },
      "source": [
        "### 5. Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqoHp30x9hH9"
      },
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVTBiOQfCPqF"
      },
      "outputs": [],
      "source": [
        "# Getting symmetric and skew symmetric features from the cplumns\n",
        "symmetric_feature=[]\n",
        "non_symmetric_feature=[]\n",
        "for i in cluster_df.describe().columns:\n",
        "  if abs(cluster_df[i].mean()-cluster_df[i].median())<0.1:\n",
        "    symmetric_feature.append(i)\n",
        "  else:\n",
        "    non_symmetric_feature.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : \",symmetric_feature)\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : \",non_symmetric_feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4yz3JmVCYe5"
      },
      "outputs": [],
      "source": [
        "#using log transformation to transform Cost as using capping tends to change median and mean\n",
        "cluster_df['Cost'] = np.log1p(cluster_df['Cost'])\n",
        "cluster_dummy['Cost'] = np.log1p(cluster_dummy['Cost'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "outputs": [],
      "source": [
        "# Transform Your data\n",
        "for i,col in enumerate(['Cost']) :\n",
        "    sns.distplot(cluster_df[col], color = '#055E85');\n",
        "    feature = cluster_df[col]\n",
        "    plt.axvline(feature.mean(), color='#ff033e', linestyle='dashed', linewidth=3,label= 'mean');  #red\n",
        "    plt.axvline(feature.median(), color='#A020F0', linestyle='dashed', linewidth=3,label='median'); #cyan\n",
        "    plt.legend(bbox_to_anchor = (1.0, 1))\n",
        "    plt.title(f'{col.title()}');\n",
        "    plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMDnDkt2B6du"
      },
      "source": [
        "### 6. Data Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "outputs": [],
      "source": [
        "# Scaling your data\n",
        "cluster_dummy.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hvr-_7lETJh"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqAtliTTDqkA"
      },
      "outputs": [],
      "source": [
        "#normalizing numerical columns\n",
        "numerical_cols = ['Cost','Total_Cuisine_Count','Average_Rating']\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(cluster_dummy[numerical_cols])\n",
        "scaled_df = cluster_dummy.copy()\n",
        "scaled_df[numerical_cols] = scaler.transform(cluster_dummy[numerical_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiiVWRdJDDil"
      },
      "source": [
        "##### Which method have you used to scale you data and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaiWauhtl6jD"
      },
      "source": [
        "We have used StandardScaler to removes the mean and scales each feature/variable to unit variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUpS68QDMuG"
      },
      "source": [
        "### 7. Dimesionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_nJCdw1JJhG"
      },
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMWYuXPjFjFF"
      },
      "outputs": [],
      "source": [
        "# print sample dataset\n",
        "scaled_df.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmlYzVQMEqOh"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqHYr2btEfrM"
      },
      "outputs": [],
      "source": [
        "\n",
        "#applying pca to for the dimensionlty deduction\n",
        "\n",
        "features = scaled_df.columns\n",
        "features = features.drop('Restaurant')\n",
        "# create an instance of PCA\n",
        "pca = PCA()\n",
        "\n",
        "# fit PCA on features\n",
        "pca.fit(scaled_df[features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNAKYEczIQt1"
      },
      "outputs": [],
      "source": [
        "#explained variance v/s no. of components\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker ='o', color = 'orange')\n",
        "plt.xlabel('number of components',size = 15, color = 'red')\n",
        "plt.ylabel('cumulative explained variance',size = 14, color = 'blue')\n",
        "plt.title('Variance v/s No. of Components',size = 20, color = 'green')\n",
        "plt.xlim([0, 8])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7s0j_80Iip5"
      },
      "outputs": [],
      "source": [
        "#using n_component as 3\n",
        "pca = PCA(n_components=3)\n",
        "\n",
        "# fit PCA on features\n",
        "pca.fit(scaled_df[features])\n",
        "\n",
        "# explained variance ratio of each principal component\n",
        "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "# variance explained by three components\n",
        "print('Cumulative variance explained by 3 principal components: {:.2%}'.format(\n",
        "                                        np.sum(pca.explained_variance_ratio_)))\n",
        "\n",
        "# transform data to principal component space\n",
        "df_pca = pca.transform(scaled_df[features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "An9iQPJ7Ir52"
      },
      "outputs": [],
      "source": [
        "#shape\n",
        "print(\"original shape: \", scaled_df.shape)\n",
        "print(\"transformed shape:\", df_pca.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqLLHVi5mbfF"
      },
      "source": [
        "Dimensionality reduction is the process of reducing the number of features in a dataset while preserving as much of the relevant information as possible. It is a technique used to overcome the curse of dimensionality, which refers to the problem of increased computational complexity and decreased performance of machine learning models as the number of features increases.\n",
        "\n",
        "There are two main types of dimensionality reduction techniques: feature selection and feature extraction.\n",
        "\n",
        "Feature selection is the process of selecting a subset of the most relevant features from the original feature set. It is a technique that helps to reduce the dimensionality of the data by removing irrelevant and redundant features. Common feature selection techniques include:\n",
        "\n",
        "* Correlation-based feature selection\n",
        "* Mutual information-based feature selection\n",
        "* Recursive feature elimination\n",
        "* SelectKBest\n",
        "\n",
        "Feature extraction is the process of creating new features from the original feature set by combining or transforming the existing features. It is a technique that helps to reduce the dimensionality of the data by creating a new feature space that is more compact and informative than the original feature space. Common feature extraction techniques include:\n",
        "\n",
        "* Principal Component Analysis (PCA)\n",
        "* Linear Discriminant Analysis (LDA)\n",
        "* Independent Component Analysis (ICA)\n",
        "* Non-Negative Matrix Factorization (NMF)\n",
        "* Autoencoder\n",
        "\n",
        "Both feature selection and feature extraction can be used to reduce the dimensionality of the data and improve the performance of machine learning models. However, the choice of technique depends on the specific task, the data, and the computational resources available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexQrXU-DjzY"
      },
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWSftDesmYIY"
      },
      "source": [
        "Yes, it is important to use dimensionality reduction techniques as dataset has 40 or more features. This is because, as the number of features increases, the computational cost of clustering algorithms also increases. In addition, high dimensionality can lead to the \"curse of dimensionality\", where the data becomes sparse and the clusters become harder to identify. Dimensionality reduction techniques such as PCA, t-SNE, or LLE can help reduce the number of features while maintaining the important information in the data, making it easier to cluster and interpret the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5CmagL3EC8N"
      },
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKr75IDuEM7t"
      },
      "source": [
        "I have used PCA as dimension reduction technique, because PCA (Principal Component Analysis) is a widely used dimensionality reduction technique because it is able to identify patterns in the data that are responsible for the most variation. These patterns, known as principal components, are linear combinations of the original features that are uncorrelated with each other. By using the first few principal components, which account for the majority of the variation in the data, one can effectively reduce the dimensionality of the data while maintaining most of the important information.\n",
        "\n",
        "Another advantage of PCA is that it is a linear technique, which means it can be applied to data that have a linear relationship between features. It is also easy to interpret the results as the principal components can be thought of as new, uncorrelated features. Additionally, PCA can be used for data visualization by projecting high-dimensional data onto a 2D or 3D space for easy visualization.\n",
        "\n",
        "\n",
        "When PCA is applied before k-means, it is used to reduce the dimensionality of the data by transforming the original feature space into a new feature space of uncorrelated principal components. The k-means algorithm is then applied to the transformed data, resulting in clusters that are defined in the new feature space. The advantage of this approach is that it can help to remove noise and correlated features from the data, which can make the clustering results more interpretable. However, it also means that the clusters may be harder to interpret in the original feature space.\n",
        "\n",
        "When PCA is applied after k-means, it is used to visualize the clusters in a lower-dimensional space. The k-means algorithm is applied to the original data, resulting in clusters that are defined in the original feature space. PCA is then used to project the data into a lower-dimensional space, making it easier to visualize and interpret the clusters. The advantage of this approach is that the clusters can be easily interpreted in the original feature space. However, it may not be as effective in removing noise and correlated features from the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhH2vgX9EjGr"
      },
      "source": [
        "### 8. Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "outputs": [],
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "#from text vectorization\n",
        "\n",
        "X = X_tfidf\n",
        "y = sentiment_df['Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOeqesheJ51A"
      },
      "outputs": [],
      "source": [
        "sentiment_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0Zrc7rYJ9_o"
      },
      "outputs": [],
      "source": [
        "#spliting test train\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "\n",
        "# describes info about train and test set\n",
        "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
        "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
        "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
        "print(\"Number transactions y_test dataset: \", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjKvONjwE8ra"
      },
      "source": [
        "##### What data splitting ratio have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      },
      "source": [
        "I have used 80:20 split which is one the most used split ratio. Since there was only 9961 data, therefore I have used more in training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XJ9OREExlT"
      },
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOzZv6IFROw"
      },
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeKDIv7pFgcC"
      },
      "source": [
        "Using class imbalance ratio (CIR) to measure data imbalance. The CIR is calculated as the ratio of the number of observations in the majority class (Nm) to the number of observations in the minority class (Nm). The CIR can be expressed as: CIR = Nm / Ns, where Nm is the number of observations in the majority class and Ns is the number of observations in the minority class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3QRone6KOFa"
      },
      "outputs": [],
      "source": [
        "#getting the value count for target class\n",
        "vc = sentiment_df.Sentiment.value_counts().reset_index().rename(columns = {'index':'Sentiment','Sentiment':'Count'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "outputs": [],
      "source": [
        "#defining majority and minority class value\n",
        "majority_class = vc.Count[0]\n",
        "minority_class = vc.Count[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_H2xRESKbmF"
      },
      "outputs": [],
      "source": [
        "#calculating cir value for checking class imbalance\n",
        "CIR = majority_class / minority_class\n",
        "CIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qihsODdWKhsQ"
      },
      "outputs": [],
      "source": [
        "# Dependant Variable Column Visualization\n",
        "sentiment_df['Sentiment'].value_counts().plot(kind='pie',figsize=(15,6),autopct=\"%1.1f%%\",startangle=90,shadow=True,\n",
        "                                              labels=['Positive Sentiment','Negative Sentiment'],colors=['purple','green'],explode=[0.01,0.02])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIqpNgepFxVj"
      },
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbet1HwdGDTz"
      },
      "source": [
        "We have found that there is imbalance in dataset with 63: 37 ratio, where 63 is the majaority class and 37 is the minority class. Even the CIR score suggest that majority class is 1.73 times greater than minority class. However it is considered as slight imbalance, therefore not performing any under or over sampling technique i.e., **not required** to treat class imabalance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCC591jGiD4"
      },
      "source": [
        "## ***7. ML Model Implementation***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      },
      "source": [
        "### **ML Model - 1 K Mean Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRPIdWV6wQ71"
      },
      "source": [
        "K-Means Clustering is an Unsupervised Learning algorithm.The algorithm takes the unlabeled dataset as input, divides the dataset into k-number of clusters, and repeats the process until it does not find the best clusters. The value of k should be predetermined in this algorithm.\n",
        "\n",
        "It is a centroid-based algorithm, where each cluster is associated with a centroid. The main aim of this algorithm is to minimize the sum of distances between the data point and their corresponding clusters.\n",
        "\n",
        "The k-means clustering algorithm mainly performs two tasks:\n",
        "\n",
        "Determines the best value for K center points or centroids by an iterative process.\n",
        "\n",
        "Assigns each data point to its closest k-center. Those data points which are near to the particular k-center, create a cluster.\n",
        "\n",
        "**ELBOW METHOD**\n",
        "\n",
        "> This method uses the concept of WCSS value. WCSS stands for Within Cluster Sum of Squares, which defines the total variations within a cluster.\n",
        "\n",
        "**SILHOUETTE METHOD**\n",
        "\n",
        "> The silhouette coefficient or silhouette score kmeans is a measure of how similar a data point is within-cluster (cohesion) compared to other clusters (separation)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "#importing kmeans\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "mOUyNIY9YEoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Within Cluster Sum of Squared Errors(WCSS) for different values of k\n",
        "wcss=[]\n",
        "for i in range(1,11):\n",
        "    km=KMeans(n_clusters=i,random_state = 20)\n",
        "    km.fit(df_pca)\n",
        "    wcss.append(km.inertia_)"
      ],
      "metadata": {
        "id": "6aTh3QQXYOx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Elbow curve\n",
        "plt.plot(range(1,11),wcss)\n",
        "plt.plot(range(1,11),wcss, linewidth=2, color=\"red\", marker =\"o\")\n",
        "plt.xlabel(\"K Value\", size = 20, color = 'purple')\n",
        "plt.xticks(np.arange(1,11,1))\n",
        "plt.ylabel(\"WCSS\", size = 20, color = 'green')\n",
        "plt.title('Elbow Curve', size = 20, color = 'blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9nWmzXm-YTPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#silhouette score\n",
        "import matplotlib.cm as cm\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import silhouette_samples\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "# candidates for the number of cluster\n",
        "parameters = list(range(2,10))\n",
        "#parameters\n",
        "parameter_grid = ParameterGrid({'n_clusters': parameters})\n",
        "best_score = -1\n",
        "#visualizing Silhouette Score for individual clusters and the clusters made\n",
        "for n_clusters in parameters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # 1st subplot is the silhouette plot\n",
        "    # silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(df_pca) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(df_pca)\n",
        "\n",
        "    # silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(df_pca, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(df_pca, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(df_pca[:, 0], df_pca[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "    #marker='$%d$' % i will give numer in cluster in 2 plot\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
        "                    s=50, edgecolor='k')\n",
        "\n",
        "    ax2.set_title(\"visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')"
      ],
      "metadata": {
        "id": "vlXbGQpEYuTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vizualizing the clusters and the datapoints in each clusters\n",
        "plt.figure(figsize = (10,6), dpi = 120)\n",
        "\n",
        "kmeans= KMeans(n_clusters = 5, init= 'k-means++', random_state = 42)\n",
        "kmeans.fit(df_pca)\n",
        "\n",
        "#predict the labels of clusters.\n",
        "label = kmeans.fit_predict(df_pca)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(df_pca[label == i , 0] , df_pca[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oJLzu8oYY1tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ML Model - 2  Hierarchicla Clustering**"
      ],
      "metadata": {
        "id": "S4z7it9BbpRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hierarchical clustering is another unsupervised machine learning algorithm, which is used to group the unlabeled datasets into a cluster and also known as hierarchical cluster analysis or HCA.\n",
        "\n",
        "In this algorithm, we develop the hierarchy of clusters in the form of a tree, and this tree-shaped structure is known as the dendrogram.\n",
        "\n"
      ],
      "metadata": {
        "id": "mvdQnIw0rY6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hierarchical clustering technique has two approaches:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " **Agglomerative :**\n",
        ">Agglomerative is a bottom-up approach, in which the algorithm starts with taking all data points as single clusters and merging them until one cluster is left.\n",
        "\n",
        "\n",
        "**Divisive :**\n",
        ">Divisive algorithm is the reverse of the agglomerative algorithm as it is a top-down approach."
      ],
      "metadata": {
        "id": "n2Wk5nqkrmoK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "outputs": [],
      "source": [
        "# Code to find hierarchial clustering and vizualizing dendograms\n",
        "\n",
        "import scipy.cluster.hierarchy as sch\n",
        "plt.figure(figsize=(12,5))\n",
        "dendrogram = sch.dendrogram(sch.linkage(df_pca, method = 'ward'),orientation='top',\n",
        "            distance_sort='descending',\n",
        "            show_leaf_counts=True)\n",
        "\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Restaurants')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adcAdS9rOdpY"
      },
      "outputs": [],
      "source": [
        "#Checking the Silhouette score for 15 clusters\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "range_n_clusters = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "for n_clusters in range_n_clusters:\n",
        "    hc = AgglomerativeClustering(n_clusters = n_clusters, affinity = 'euclidean', linkage = 'ward')\n",
        "    y_hc = hc.fit_predict(df_pca)\n",
        "    score = silhouette_score(df_pca, y_hc)\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGcMbQlJOm93"
      },
      "outputs": [],
      "source": [
        "# agglomerative clustering\n",
        "from numpy import unique\n",
        "from numpy import where\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# define the model\n",
        "model = AgglomerativeClustering(n_clusters = 5)      #n_clusters=5\n",
        "# fit model and predict clusters\n",
        "y_hc = model.fit_predict(df_pca)\n",
        "# retrieve unique clusters\n",
        "clusters = unique(y_hc)\n",
        "# create scatter plot for samples from each cluster\n",
        "for cluster in clusters:\n",
        "\t# get row indexes for samples with this cluster\n",
        "\trow_ix = where(y_hc == cluster)\n",
        "\t# create scatter of these samples\n",
        "\tplt.scatter(df_pca[row_ix, 0], df_pca[row_ix, 1])\n",
        "# show the plot\n",
        "plt.show()\n",
        "#Evaluation\n",
        "\n",
        "#Silhouette Coefficient\n",
        "print(\"Silhouette Coefficient: %0.3f\"%silhouette_score(df_pca,y_hc, metric='euclidean'))\n",
        "\n",
        "#davies_bouldin_score of our clusters\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "davies_bouldin_score(df_pca, y_hc)\n",
        "print(\"davies_bouldin_score %0.3f\"%davies_bouldin_score(df_pca, y_hc))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "c9Ul6pA7y6CU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KMeans Clustering**\n",
        "> I applied K means Clustering to cluster the Restaurants based on the given features. I used both the Elbow and Silhuoette Methods to get an efficient number of K, and we discovered that n clusters = 6 was best for our model. The model was then fitted using K means, and each data point was labelled with the cluster to which it belonged using K means.labels. After labelling the clusters, we visualised them and counted the number of restaurants in each cluster, discovering that the majority of the restaurants belonged to the first cluster.\n",
        "\n",
        "**Agglomerative Hierarchical Clustering**\n",
        "> I have used Hierarchial Clustering - Agglomerative Model to cluster the restaurants based on different features. This model uses a down-top approach to cluster the data. I have used Silhouette Coefficient Score and used clusters = 6 and then vizualized the clusters and the datapoints within it."
      ],
      "metadata": {
        "id": "7eEhazEryzTD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      },
      "source": [
        "### **ML Model - 3 Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er2zHC44fAjx"
      },
      "source": [
        "**Linear Discrimination Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Discriminant analysis is one of the most popular dimensionality reduction techniques used for supervised classification problems in machine learning. It is also considered a pre-processing step for modeling differences in ML and applications of pattern classification."
      ],
      "metadata": {
        "id": "fsKJla9GxUwK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEOv_Y-JjlXX"
      },
      "outputs": [],
      "source": [
        "#Import pyldavis to visualise\n",
        "!pip install pyLDAvis\n",
        "!pip install scikit-learn\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "pyLDAvis.enable_notebook()\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCHrU-I5e3ix"
      },
      "outputs": [],
      "source": [
        "#calculating silhouette score for n_component\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "topic_range = range(2, 11)\n",
        "silhouette_scores = []\n",
        "\n",
        "for n_components in topic_range:\n",
        "    lda = LatentDirichletAllocation(n_components=n_components)\n",
        "    lda.fit(X)\n",
        "    labels = lda.transform(X).argmax(axis=1)\n",
        "    silhouette_scores.append(silhouette_score(X, labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vibkj3cagbOz"
      },
      "outputs": [],
      "source": [
        "#plotting silhouette score\n",
        "plt.plot(topic_range, silhouette_scores, marker ='o', color='red')\n",
        "plt.xlabel('Number of Topics', size = 15, color = 'green')\n",
        "plt.ylabel('Silhouette Score', size = 15, color = 'blue')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.corpus"
      ],
      "metadata": {
        "id": "CumqLoYQmdwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5OotmbNTixp"
      },
      "outputs": [],
      "source": [
        "# LDA model\n",
        "lda = LatentDirichletAllocation(n_components=4)\n",
        "\n",
        "lda.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9TLr-gYbF0I"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "na9ezc40YOpb"
      },
      "outputs": [],
      "source": [
        "# ploting the clusters top 30 terms\n",
        "lda_pyLDAvis = pyLDAvis.sklearn.prepare(lda, X, vectorizer, mds='tsne')\n",
        "lda_pyLDAvis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* LDA is an unsupervised learning algorithm, it doesn't have any predefined labels. The labels are assigned based on the analysis done on the words, the weights of the words, and the context of the words in each topic.\n",
        "\n",
        " So, the predicted topic is not a definite answer, therfore experimenting with different techniques like using supervised algorithm and combining the results to make a more accurate sentiment labeling.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4sIHNjKyzGFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ML Model - 4     Logistic Regression**"
      ],
      "metadata": {
        "id": "irSJsORwjAym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining function to calculate score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from tabulate import tabulate\n",
        "import itertools\n",
        "\n",
        "\n",
        "#calculating score\n",
        "def calculate_scores(model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    # Get the confusion matrix for both train and test\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.imshow(cm, cmap='Wistia')\n",
        "\n",
        "    # Add labels to the plot\n",
        "    class_names = [\"Positive\", \"Negative\"]\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    # Add values inside the confusion matrix\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    # Add a title and x and y labels\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.ylabel('True label')\n",
        "\n",
        "    plt.show()\n",
        "    print(cm)\n",
        "    return roc_auc, f1, accuracy, precision, recall\n",
        "\n",
        "#printing result\n",
        "def print_table(model, X_train, y_train, X_test, y_test):\n",
        "    roc_auc, f1, accuracy, precision, recall = calculate_scores(model, X_train, y_train, X_test, y_test)\n",
        "    table = [[\"ROC AUC\", roc_auc], [\"Precision\", precision],\n",
        "             [\"Recall\", recall], [\"F1\", f1], [\"Accuracy\", accuracy]]\n",
        "    print(tabulate(table, headers=[\"Metric\", \"Score\"]))"
      ],
      "metadata": {
        "id": "wCS9B1isffsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**"
      ],
      "metadata": {
        "id": "4pt5u6_yxeyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is a statistical method that is used for building machine learning models where the dependent variable is dichotomous: i.e. binary. Logistic regression is used to describe data and the relationship between one dependent variable and one or more independent variables. The independent variables can be nominal, ordinal, or of interval type.\n",
        "\n",
        "\n",
        "The name logistic regression is derived from the concept of the logistic function that it uses. The logistic function is also known as the sigmoid function. The value of this logistic function lies between zero and one."
      ],
      "metadata": {
        "id": "MjcGj7_us_yh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The name logistic regression is derived from the concept of the logistic function that it uses. The logistic function is also known as the sigmoid function. The value of this logistic function lies between zero and one."
      ],
      "metadata": {
        "id": "MU4YqRlYtsj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#logisctic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "# create and fit the model\n",
        "clf = LogisticRegression()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mPFJ8NGtf0Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWYfwnehpsJ1"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart for logistic regression\n",
        "\n",
        "print_table(clf, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first row of the matrix represents the predicted positive class (1) and the second row represents the predicted negative class (0). The first column represents the actual positive class (1) and the second column represents the actual negative class (0).\n",
        "\n",
        "* 597 instances are labeled as True Positive (correctly predicted as positive)\n",
        "* 176 instances are labeled as False Positive (incorrectly predicted as positive)\n",
        "* 1136 instances are labeled as True Negative (correctly predicted as negative)\n",
        "* 84 instances are labeled as False Negative (incorrectly predicted as negative)"
      ],
      "metadata": {
        "id": "xGH2IWRKzOBF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Checking the best parameters for LogisticRegression by gridsearchcv\n",
        "param_dict = {'C': [0.1,1,10,100,1000],'penalty': ['l1', 'l2'],'max_iter':[1000]}\n",
        "clf_grid = GridSearchCV(clf, param_dict,n_jobs=-1, cv=5, verbose = 5,scoring='recall')\n",
        "\n",
        "# Predict on the model\n",
        "print_table(clf_grid, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "rmpTB9P4xWfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first row of the matrix represents the predicted positive class (1) and the second row represents the predicted negative class (0). The first column represents the actual positive class (1) and the second column represents the actual negative class (0).\n",
        "\n",
        "* 451 instances are labeled as True Positive (correctly predicted as positive)\n",
        "* 322 instances are labeled as False Positive (incorrectly predicted as positive)\n",
        "* 1177 instances are labeled as True Negative (correctly predicted as negative)\n",
        "* 43 instances are labeled as False Negative (incorrectly predicted as negative)"
      ],
      "metadata": {
        "id": "mUh9CPcE090v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAih1iBOpsJ2"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      },
      "source": [
        "We have used GridSearchCV as the hyperparameter optimization technique as it uses all possible combinations of hyperparameters and provides the more accurate results. It then calculates the performance for each combination and selects the best value for the hyperparameters. This offers the most accurate tuning method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      },
      "source": [
        "#### Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used these metrices for evaluation of the model and their impact on business are as follows:\n",
        "\n",
        "Accuracy: This metric indicates the percentage of correctly classified instances out of the total number of instances. In a business setting, this would indicate the overall effectiveness of the model in making correct predictions. A high accuracy score would have a positive impact on the business, as it would indicate a high level of confidence in the model's predictions.\n",
        "\n",
        "Precision: This metric indicates the proportion of true positive predictions out of all positive predictions made by the model. In a business setting, this would indicate the level of confidence in the model's ability to identify positive instances correctly. A high precision score would have a positive impact on the business, as it would indicate that the model is not making false positive predictions.\n",
        "\n",
        "Recall: This metric indicates the proportion of true positive predictions out of all actual positive instances. In a business setting, this would indicate the model's ability to identify all positive instances. A high recall score would have a positive impact on the business, as it would indicate that the model is not missing any positive instances.\n",
        "\n",
        "F1 Score: This metric is a combination of precision and recall and is used to balance the trade-off between the two. In a business setting, this would indicate the overall effectiveness of the model in making correct predictions while also avoiding false positives and false negatives. A high F1 score would have a positive impact on the business, as it would indicate that the model is making accurate predictions while also being able to identify all positive instances.\n",
        "\n",
        "ROC AUC: This metric indicates the ability of the model to distinguish between positive and negative instances. In a business setting, this would indicate the model's ability to correctly classify instances as positive or negative. A high ROC AUC score would have a positive impact on the business, as it would indicate that the model is able to correctly classify instances.\n",
        "\n",
        "The XgBoost Classifier can be considered as an efficient model for the business, especially when it achieves high scores in all of these evaluation metrics, which would indicate that it can accurately predict outcomes, identify all positive instances, and correctly classify instances as positive or negative."
      ],
      "metadata": {
        "id": "Q1qn8y5WyUYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ML Model - 5   XGBoost**"
      ],
      "metadata": {
        "id": "OXu52uwPybFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart for XgBoost\n",
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier()\n",
        "# printing result\n",
        "print_table(xgb, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "Y-zoD-WAyakr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameters for XGBRegressor by gridsearchcv\n",
        "xgb_param={'n_estimators': [100,125,150],'max_depth': [7,10,15],'criterion': ['entropy']}\n",
        "xgb_grid=GridSearchCV(estimator=xgb,param_grid = xgb_param,cv=3,scoring='recall',verbose=5,n_jobs = -1)\n",
        "\n",
        "# printing result for gridsearch Xgb\n",
        "print_table(xgb_grid, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "8PBM537c2YbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "xnx0Z89t3yuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used GridSearchCV as the hyperparameter optimization technique as it uses all possible combinations of hyperparameters and provides the more accurate results. It then calculates the performance for each combination and selects the best value for the hyperparameters. This offers the most accurate tuning method"
      ],
      "metadata": {
        "id": "DWu10GRJ30H0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "EnuCwoWr1I57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used these metrices for evaluation of the model and their impact on business are as follows:\n",
        "\n",
        "**Accuracy:** This metric indicates the percentage of correctly classified instances out of the total number of instances. In a business setting, this would indicate the overall effectiveness of the model in making correct predictions. A high accuracy score would have a positive impact on the business, as it would indicate a high level of confidence in the model's predictions.\n",
        "\n",
        "**Precision :** This metric indicates the proportion of true positive predictions out of all positive predictions made by the model. In a business setting, this would indicate the level of confidence in the model's ability to identify positive instances correctly. A high precision score would have a positive impact on the business, as it would indicate that the model is not making false positive predictions.\n",
        "\n",
        "**Recall:** This metric indicates the proportion of true positive predictions out of all actual positive instances. In a business setting, this would indicate the model's ability to identify all positive instances. A high recall score would have a positive impact on the business, as it would indicate that the model is not missing any positive instances.\n",
        "\n",
        "**F1 Score:** This metric is a combination of precision and recall and is used to balance the trade-off between the two. In a business setting, this would indicate the overall effectiveness of the model in making correct predictions while also avoiding false positives and false negatives. A high F1 score would have a positive impact on the business, as it would indicate that the model is making accurate predictions while also being able to identify all positive instances.\n",
        "\n",
        "**ROC AUC:** This metric indicates the ability of the model to distinguish between positive and negative instances. In a business setting, this would indicate the model's ability to correctly classify instances as positive or negative. A high ROC AUC score would have a positive impact on the business, as it would indicate that the model is able to correctly classify instances.\n",
        "\n",
        "The XgBoost Classifier can be considered as an efficient model for the business, especially when it achieves high scores in all of these evaluation metrics, which would indicate that it can accurately predict outcomes, identify all positive instances, and correctly classify instances as positive or negative."
      ],
      "metadata": {
        "id": "K0gJVzhc1GsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering and sentiment analysis were performed on a dataset of customer reviews for the food delivery service Zomato. The purpose of this analysis was to understand the customer's experience and gain insights about their feedback.\n",
        "\n",
        "The clustering technique was applied to group customers based on their review text, and it was found that the customers were grouped into two clusters: positive and negative. This provided a general understanding of customer satisfaction levels, with the positive cluster indicating the highest level of satisfaction and the negative cluster indicating the lowest level of satisfaction.\n",
        "\n",
        "Sentiment analysis was then applied to classify the review text as positive or negative. This provided a more detailed understanding of customer feedback and helped to identify specific areas where the service could be improved.\n",
        "\n",
        "Overall, this analysis provided valuable insights into the customer's experience with Zomato, and it could be used to guide future business decisions and improve the service. Additionally, by combining clustering and sentiment analysis techniques, a more comprehensive understanding of customer feedback was achieved.\n",
        "\n",
        "Other important discoveries during analysis are -\n",
        "* AB's - Absolute Barbecues, show maximum engagement and retention as it has maximum number of rating on average and Hotel Zara Hi-Fi show lowest engagement as has lowest average rating.\n",
        "\n",
        "* Price point for high rated hotel AB's= Absolute Barbecues is 1500 and price point for low rated restaurant Hotel Zara Hi-Fi is 400.\n",
        "\n",
        "* North Indian food followed by chinese are best or indeemand food as sold by most of the restaurants.\n",
        "\n",
        "* Great Buffets is the most frequently used tags and other tags like great, best, north, Hyderabad is also used in large quantity.\n",
        "\n",
        "* Satwinder singh is the most popular critic who has maximum number of follower and on an average he give 3.5 rating.\n",
        "\n",
        "* restaurant Collage - Hyatt Hyderabad Gachibowli is most expensive restaurant in the locality which has a price of 2800 for order and has 3.5 average rating. Hotels like Amul and Mohammedia Shawarma are least expensive with price of 150 and has 3.9 average rating.\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}